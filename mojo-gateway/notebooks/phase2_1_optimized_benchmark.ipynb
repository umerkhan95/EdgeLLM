{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 2.1: Optimized Kernels Benchmark\n",
    "\n",
    "This notebook tests Phase 2.1 optimizations that fix the Phase 2 regression:\n",
    "\n",
    "**Phase 2 Issues Identified:**\n",
    "- \"Fused\" kernel wasn't truly fused (stored intermediate in shared memory)\n",
    "- atomicAdd in T-MAC LUT causing contention\n",
    "- Overhead from stream sync and pinned memory for small tensors\n",
    "\n",
    "**Phase 2.1 Solutions:**\n",
    "1. **Warp-private accumulation** - No atomicAdd (each thread accumulates privately)\n",
    "2. **True streaming fusion** - Normalize on-the-fly without intermediate storage\n",
    "3. **Adaptive dispatch** - Choose best kernel based on tensor size\n",
    "4. **Warp-level shuffle reduction** - Fast reduction using `__shfl_down_sync`\n",
    "\n",
    "**Target**: Fix Phase 2 regression, achieve 150+ tok/s (baseline before: 80 tok/s)\n",
    "\n",
    "**Requirements:**\n",
    "- NVIDIA GPU (T4, RTX, Jetson)\n",
    "- CUDA Toolkit 11.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "if not os.path.exists('ollama-api-gateway'):\n",
    "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "else:\n",
    "    print('Repository exists, pulling latest...')\n",
    "    !cd ollama-api-gateway && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CUDA kernels with Phase 2.1 optimizations\n",
    "%cd ollama-api-gateway/mojo-gateway/src/kernels\n",
    "!make cuda\n",
    "!ls -la ../../lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Load CUDA Library with Phase 2.1 APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load library\n",
    "lib_path = '../../lib/libtmac_kernel_cuda.so'\n",
    "cuda_lib = ctypes.CDLL(lib_path)\n",
    "print(f'Loaded: {lib_path}')\n",
    "\n",
    "# Basic functions\n",
    "cuda_lib.cuda_available.restype = ctypes.c_int\n",
    "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
    "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
    "cuda_lib.cuda_init.restype = ctypes.c_int\n",
    "cuda_lib.cuda_cleanup.restype = None\n",
    "cuda_lib.cuda_sync.restype = None\n",
    "\n",
    "# Phase 1: Persistent memory API\n",
    "cuda_lib.cuda_load_weights.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_int8), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int\n",
    "]\n",
    "cuda_lib.cuda_load_weights.restype = ctypes.c_int\n",
    "cuda_lib.cuda_unload_weights.restype = None\n",
    "cuda_lib.cuda_weights_loaded.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.tmac_matmul_cuda_persistent.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
    "]\n",
    "cuda_lib.tmac_matmul_cuda_persistent.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.cuda_load_norm_weights.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.c_int]\n",
    "cuda_lib.cuda_load_norm_weights.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.rmsnorm_cuda_persistent.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.rmsnorm_cuda_persistent.restype = ctypes.c_int\n",
    "\n",
    "# Phase 2: Original fused kernels (for comparison)\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda.restype = ctypes.c_int\n",
    "\n",
    "# Phase 2.1: NEW Optimized kernels\n",
    "# V3 kernel (warp-private accumulation, no atomicAdd)\n",
    "cuda_lib.tmac_matmul_cuda_v3.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
    "]\n",
    "cuda_lib.tmac_matmul_cuda_v3.restype = ctypes.c_int\n",
    "\n",
    "# Streaming fused (true fusion, normalize on-the-fly)\n",
    "cuda_lib.streaming_fused_rmsnorm_matmul_cuda.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.streaming_fused_rmsnorm_matmul_cuda.restype = ctypes.c_int\n",
    "\n",
    "# Adaptive dispatch (auto-selects best kernel)\n",
    "cuda_lib.tmac_matmul_cuda_adaptive.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
    "]\n",
    "cuda_lib.tmac_matmul_cuda_adaptive.restype = ctypes.c_int\n",
    "\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_adaptive.argtypes = [\n",
    "    ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_float),\n",
    "    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_float\n",
    "]\n",
    "cuda_lib.fused_rmsnorm_matmul_cuda_adaptive.restype = ctypes.c_int\n",
    "\n",
    "print('Function signatures defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA and initialize\n",
    "if cuda_lib.cuda_available():\n",
    "    device_name = cuda_lib.cuda_device_name().decode('utf-8')\n",
    "    print(f'CUDA Device: {device_name}')\n",
    "else:\n",
    "    raise RuntimeError('No CUDA device found!')\n",
    "\n",
    "# Initialize with generous buffer sizes\n",
    "max_weights = 100_000_000  # 100MB for weights\n",
    "max_activations = 10_000_000\n",
    "max_output = 10_000_000\n",
    "ret = cuda_lib.cuda_init(max_weights, max_activations, max_output)\n",
    "if ret == 0:\n",
    "    print('CUDA initialized successfully')\n",
    "else:\n",
    "    raise RuntimeError('CUDA initialization failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Model Parameters (SmolLM-135M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmolLM-135M architecture\n",
    "hidden_size = 576\n",
    "intermediate_size = 1536\n",
    "num_heads = 9\n",
    "head_dim = hidden_size // num_heads  # 64\n",
    "vocab_size = 49152\n",
    "num_layers = 9\n",
    "\n",
    "print(f'Hidden size: {hidden_size}')\n",
    "print(f'Intermediate size: {intermediate_size}')\n",
    "print(f'Num layers: {num_layers}')\n",
    "print(f'\\nTest tensor sizes (M*K):')\n",
    "print(f'  QKV Projection: {3*hidden_size} x {hidden_size} = {3*hidden_size*hidden_size:,}')\n",
    "print(f'  FFN Up:         {intermediate_size} x {hidden_size} = {intermediate_size*hidden_size:,}')\n",
    "print(f'  FFN Down:       {hidden_size} x {intermediate_size} = {hidden_size*intermediate_size:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_phase1_separate(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 1: Separate RMSNorm + MatMul calls.\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    norm_out = np.zeros(K, dtype=np.float32)\n",
    "    norm_out_ptr = norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_persistent(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000\n",
    "\n",
    "def benchmark_phase2_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2: Original fused kernel (has atomicAdd issues).\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000\n",
    "\n",
    "def benchmark_v3_kernel(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2.1: V3 kernel (warp-private accumulation, no atomicAdd).\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    norm_out = np.zeros(K, dtype=np.float32)\n",
    "    norm_out_ptr = norm_out.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_v3(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.rmsnorm_cuda_persistent(norm_out_ptr, act_ptr, 1, K, ctypes.c_float(1e-6))\n",
    "        cuda_lib.tmac_matmul_cuda_v3(norm_out_ptr, out_ptr, M, N, K)\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000\n",
    "\n",
    "def benchmark_streaming_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2.1: Streaming fused kernel (true fusion, normalize on-the-fly).\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.streaming_fused_rmsnorm_matmul_cuda(act_ptr, out_ptr, M, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000\n",
    "\n",
    "def benchmark_adaptive(weights, activations, output, scales, norm_weights, M, N, K, iterations=100):\n",
    "    \"\"\"Phase 2.1: Adaptive dispatch (auto-selects best kernel).\"\"\"\n",
    "    weights_ptr = weights.ctypes.data_as(ctypes.POINTER(ctypes.c_int8))\n",
    "    act_ptr = activations.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    out_ptr = output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    scales_ptr = scales.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    norm_ptr = norm_weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float))\n",
    "    \n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    cuda_lib.cuda_load_weights(weights_ptr, scales_ptr, weight_bytes, M)\n",
    "    cuda_lib.cuda_load_norm_weights(norm_ptr, K)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_adaptive(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        cuda_lib.fused_rmsnorm_matmul_cuda_adaptive(act_ptr, out_ptr, M, N, K, ctypes.c_float(1e-6))\n",
    "    cuda_lib.cuda_sync()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    cuda_lib.cuda_unload_weights()\n",
    "    return (end - start) / iterations * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Benchmarks: Phase 1 vs Phase 2 vs Phase 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configurations simulating transformer layers\n",
    "test_configs = [\n",
    "    ('QKV Projection', 3 * hidden_size, 1, hidden_size),\n",
    "    ('Output Projection', hidden_size, 1, hidden_size),\n",
    "    ('FFN Up', intermediate_size, 1, hidden_size),\n",
    "    ('FFN Down', hidden_size, 1, intermediate_size),\n",
    "]\n",
    "\n",
    "print('=' * 130)\n",
    "print('PHASE 2.1 BENCHMARK: Optimized Kernels (No Atomics, True Fusion)')\n",
    "print('=' * 130)\n",
    "print(f'{\"Layer\":<18} {\"M*K\":>10} {\"Phase1\":>10} {\"Phase2\":>10} {\"V3\":>10} {\"Stream\":>10} {\"Adaptive\":>10} {\"Best\":>10} {\"vs Phase1\":>10}')\n",
    "print(f'{\"\":<18} {\"\":>10} {\"(ms)\":>10} {\"(ms)\":>10} {\"(ms)\":>10} {\"(ms)\":>10} {\"(ms)\":>10} {\"\":>10} {\"\":>10}')\n",
    "print('-' * 130)\n",
    "\n",
    "results = []\n",
    "iterations = 100\n",
    "\n",
    "for name, M, N, K in test_configs:\n",
    "    weight_bytes = M * ((K + 3) // 4)\n",
    "    weights = np.random.randint(-1, 2, size=weight_bytes, dtype=np.int8)\n",
    "    activations = np.random.randn(K * N).astype(np.float32)\n",
    "    output = np.zeros(M * N, dtype=np.float32)\n",
    "    scales = np.ones(M, dtype=np.float32)\n",
    "    norm_weights = np.ones(K, dtype=np.float32)\n",
    "    \n",
    "    try:\n",
    "        phase1_ms = benchmark_phase1_separate(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        phase2_ms = benchmark_phase2_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        v3_ms = benchmark_v3_kernel(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        stream_ms = benchmark_streaming_fused(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        adaptive_ms = benchmark_adaptive(weights, activations, output, scales, norm_weights, M, N, K, iterations)\n",
    "        \n",
    "        best_ms = min(phase1_ms, phase2_ms, v3_ms, stream_ms, adaptive_ms)\n",
    "        best_name = ['Phase1', 'Phase2', 'V3', 'Stream', 'Adaptive'][[phase1_ms, phase2_ms, v3_ms, stream_ms, adaptive_ms].index(best_ms)]\n",
    "        speedup = phase1_ms / best_ms\n",
    "        \n",
    "        print(f'{name:<18} {M*K:>10,} {phase1_ms:>10.3f} {phase2_ms:>10.3f} {v3_ms:>10.3f} {stream_ms:>10.3f} {adaptive_ms:>10.3f} {best_name:>10} {speedup:>9.2f}x')\n",
    "        results.append((name, M, K, phase1_ms, phase2_ms, v3_ms, stream_ms, adaptive_ms, best_ms, speedup))\n",
    "    except Exception as e:\n",
    "        print(f'{name:<18} ERROR: {e}')\n",
    "\n",
    "print('-' * 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print('\\n' + '=' * 80)\n",
    "    print('ANALYSIS: Which kernel is fastest for each layer?')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    for r in results:\n",
    "        name, M, K, p1, p2, v3, stream, adaptive, best, speedup = r\n",
    "        print(f'\\n{name}:')\n",
    "        print(f'  Phase 1 (separate):     {p1:.3f} ms')\n",
    "        print(f'  Phase 2 (atomicAdd):    {p2:.3f} ms ({p1/p2:.2f}x vs Phase1)')\n",
    "        print(f'  V3 (warp-private):      {v3:.3f} ms ({p1/v3:.2f}x vs Phase1)')\n",
    "        print(f'  Streaming (true fuse):  {stream:.3f} ms ({p1/stream:.2f}x vs Phase1)')\n",
    "        print(f'  Adaptive:               {adaptive:.3f} ms ({p1/adaptive:.2f}x vs Phase1)')\n",
    "        print(f'  --> Best: {best:.3f} ms ({speedup:.2f}x speedup)')\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_phase1 = sum(r[3] for r in results)\n",
    "    total_phase2 = sum(r[4] for r in results)\n",
    "    total_v3 = sum(r[5] for r in results)\n",
    "    total_stream = sum(r[6] for r in results)\n",
    "    total_adaptive = sum(r[7] for r in results)\n",
    "    \n",
    "    print('\\n' + '=' * 80)\n",
    "    print('TOTAL TIME PER LAYER (all projections):')\n",
    "    print('=' * 80)\n",
    "    print(f'Phase 1:  {total_phase1:.3f} ms')\n",
    "    print(f'Phase 2:  {total_phase2:.3f} ms ({total_phase1/total_phase2:.2f}x)')\n",
    "    print(f'V3:       {total_v3:.3f} ms ({total_phase1/total_v3:.2f}x)')\n",
    "    print(f'Streaming:{total_stream:.3f} ms ({total_phase1/total_stream:.2f}x)')\n",
    "    print(f'Adaptive: {total_adaptive:.3f} ms ({total_phase1/total_adaptive:.2f}x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Estimated Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Use adaptive kernel for final estimate\n",
    "    total_adaptive = sum(r[7] for r in results)\n",
    "    total_phase1 = sum(r[3] for r in results)\n",
    "    \n",
    "    # Per-token time = layer_time * num_layers\n",
    "    token_time_phase1 = total_phase1 * num_layers\n",
    "    token_time_adaptive = total_adaptive * num_layers\n",
    "    \n",
    "    tok_s_phase1 = 1000 / token_time_phase1\n",
    "    tok_s_adaptive = 1000 / token_time_adaptive\n",
    "    \n",
    "    print('\\n' + '=' * 80)\n",
    "    print('ESTIMATED THROUGHPUT (SmolLM-135M, 9 layers)')\n",
    "    print('=' * 80)\n",
    "    \n",
    "    print(f'\\nPhase 1 (Persistent Memory):')\n",
    "    print(f'  Per token: {token_time_phase1:.2f} ms')\n",
    "    print(f'  Throughput: {tok_s_phase1:.1f} tok/s')\n",
    "    \n",
    "    print(f'\\nPhase 2.1 (Adaptive Dispatch):')\n",
    "    print(f'  Per token: {token_time_adaptive:.2f} ms')\n",
    "    print(f'  Throughput: {tok_s_adaptive:.1f} tok/s')\n",
    "    \n",
    "    print(f'\\nImprovement: {tok_s_adaptive/tok_s_phase1:.2f}x faster than Phase 1')\n",
    "    \n",
    "    OLLAMA_TOK_S = 423\n",
    "    print(f'\\n' + '-' * 80)\n",
    "    print(f'Ollama comparison:')\n",
    "    print(f'  EdgeLLM Phase 2.1: {tok_s_adaptive:.1f} tok/s')\n",
    "    print(f'  Ollama:            {OLLAMA_TOK_S} tok/s')\n",
    "    print(f'  Gap:               {OLLAMA_TOK_S/tok_s_adaptive:.1f}x')\n",
    "    print(f'  Progress:          {tok_s_adaptive/OLLAMA_TOK_S*100:.1f}% of Ollama speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_lib.cuda_cleanup()\n",
    "print('CUDA resources cleaned up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Phase 2.1 Optimizations:\n",
    "\n",
    "| Kernel | Key Feature | Best For |\n",
    "|--------|-------------|----------|\n",
    "| V3 | Warp-private accumulation, no atomicAdd | Medium tensors |\n",
    "| Streaming | True fusion (normalize on-the-fly) | Large tensors, batch=1 |\n",
    "| Adaptive | Auto-selects best kernel | All cases |\n",
    "\n",
    "### Phase 2 vs Phase 2.1:\n",
    "\n",
    "| Issue | Phase 2 Problem | Phase 2.1 Solution |\n",
    "|-------|-----------------|--------------------|\n",
    "| Fusion | Stored intermediate in shared mem | Normalize on-the-fly |\n",
    "| Atomics | atomicAdd contention | Warp-private + shuffle reduction |\n",
    "| Dispatch | One-size-fits-all | Adaptive based on tensor size |\n",
    "\n",
    "### Next Steps:\n",
    "- Profile with Nsight Compute to identify remaining bottlenecks\n",
    "- Consider INT8 tensor core path for large matrices\n",
    "- Implement CUDA Graphs for full forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
