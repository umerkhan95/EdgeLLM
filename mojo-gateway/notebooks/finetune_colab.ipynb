{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# EdgeLLM Fine-Tuning on Google Colab\n",
        "\n",
        "**Fine-tune your own LLM for FREE on Google Colab!**\n",
        "\n",
        "This notebook walks you through:\n",
        "1. Setting up the environment\n",
        "2. Preparing your dataset\n",
        "3. Fine-tuning with QLoRA\n",
        "4. Merging and quantizing the model\n",
        "5. Deploying to edge devices\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab (FREE tier works!)\n",
        "- T4 GPU (free) or better\n",
        "- ~15 minutes for a small model\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "First, let's install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers peft bitsandbytes datasets accelerate trl\n",
        "!pip install -q sentencepiece protobuf\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_selection"
      },
      "source": [
        "## 2. Select Base Model\n",
        "\n",
        "Choose a base model based on your target hardware:\n",
        "\n",
        "| Model | Size | BitNet | Hardware | Speed |\n",
        "|-------|------|--------|----------|-------|\n",
        "| SmolLM-135M | 135M | 35MB | Pi Zero 2 W | 5-10 tok/s |\n",
        "| SmolLM-360M | 360M | 90MB | Pi Zero 2 W | 3-6 tok/s |\n",
        "| Qwen2-0.5B | 500M | 125MB | Pi 4 | 8-15 tok/s |\n",
        "| Llama-3.2-1B | 1B | 200MB | Pi 5 | 20-40 tok/s |\n",
        "| Phi-3-mini | 3.8B | 750MB | Jetson | 10-20 tok/s |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_config"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_ID = \"HuggingFaceTB/SmolLM-135M\"  # Change this!\n",
        "OUTPUT_DIR = \"./finetuned_model\"\n",
        "\n",
        "# Training settings\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "# LoRA settings\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## 3. Prepare Your Dataset\n",
        "\n",
        "You can either:\n",
        "1. Upload your own JSONL file\n",
        "2. Use a HuggingFace dataset\n",
        "3. Create data directly in the notebook\n",
        "\n",
        "### Data Format (JSONL)\n",
        "```json\n",
        "{\"instruction\": \"What is 2+2?\", \"response\": \"4\"}\n",
        "{\"instruction\": \"Say hello\", \"response\": \"Hello! How can I help you?\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_data"
      },
      "outputs": [],
      "source": [
        "# Option 1: Create sample data directly\n",
        "sample_data = [\n",
        "    {\"instruction\": \"Turn on the living room lights\", \"response\": \"Turning on the living room lights now.\"},\n",
        "    {\"instruction\": \"What's the temperature?\", \"response\": \"The current temperature is 72°F (22°C).\"},\n",
        "    {\"instruction\": \"Set a timer for 5 minutes\", \"response\": \"Timer set for 5 minutes. I'll let you know when it's done.\"},\n",
        "    {\"instruction\": \"Play some music\", \"response\": \"Playing your favorite playlist now.\"},\n",
        "    {\"instruction\": \"What's on my calendar today?\", \"response\": \"You have a meeting at 2 PM and a dentist appointment at 4 PM.\"},\n",
        "    # Add more examples here!\n",
        "]\n",
        "\n",
        "# Save as JSONL\n",
        "with open(\"dataset.jsonl\", \"w\") as f:\n",
        "    for item in sample_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"Created dataset with {len(sample_data)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Option 2: Upload your own file\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Upload your dataset.jsonl\n",
        "\n",
        "# Load dataset\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "dataset = load_jsonl(\"dataset.jsonl\")\n",
        "print(f\"Loaded {len(dataset)} examples\")\n",
        "print(f\"\\nSample: {dataset[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "## 4. Load Model and Train\n",
        "\n",
        "We use QLoRA for efficient fine-tuning:\n",
        "- 4-bit quantized base model\n",
        "- LoRA adapters for training\n",
        "- Uses only ~4GB VRAM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model: {MODEL_ID}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "configure_lora"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "formatting"
      },
      "outputs": [],
      "source": [
        "# Format dataset for training\n",
        "def format_instruction(example):\n",
        "    if \"input\" in example and example[\"input\"]:\n",
        "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['response']}\"\n",
        "    else:\n",
        "        text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    formatting_func=format_instruction,\n",
        "    max_seq_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_header"
      },
      "source": [
        "## 5. Test Your Model\n",
        "\n",
        "Let's test the fine-tuned model before quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "def generate(prompt, max_tokens=50):\n",
        "    inputs = tokenizer(f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\", return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Turn on the kitchen lights\",\n",
        "    \"What time is it?\",\n",
        "    \"Set an alarm for 7 AM\",\n",
        "]\n",
        "\n",
        "print(\"Testing model...\\n\")\n",
        "for prompt in test_prompts:\n",
        "    print(f\"User: {prompt}\")\n",
        "    response = generate(prompt)\n",
        "    # Extract just the response part\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.split(\"### Response:\")[1].strip()\n",
        "    print(f\"Assistant: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merge_header"
      },
      "source": [
        "## 6. Merge LoRA Weights\n",
        "\n",
        "Merge the LoRA adapters back into the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load base model (full precision)\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cpu\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load and merge LoRA\n",
        "print(\"Merging LoRA weights...\")\n",
        "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "MERGED_DIR = f\"{OUTPUT_DIR}_merged\"\n",
        "merged_model.save_pretrained(MERGED_DIR)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print(f\"Merged model saved to {MERGED_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quantize_header"
      },
      "source": [
        "## 7. Quantize to BitNet (Optional)\n",
        "\n",
        "Convert to BitNet 1.58-bit for edge deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quantize"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "\n",
        "def quantize_to_bitnet(model, output_path):\n",
        "    \"\"\"Simple BitNet quantization.\"\"\"\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        # Write header\n",
        "        f.write(b\"TMAC\")\n",
        "        f.write(struct.pack(\"I\", 2))  # version\n",
        "        \n",
        "        config = model.config\n",
        "        f.write(struct.pack(\"I\", config.hidden_size))\n",
        "        f.write(struct.pack(\"I\", config.num_hidden_layers))\n",
        "        f.write(struct.pack(\"I\", config.num_attention_heads))\n",
        "        f.write(struct.pack(\"I\", config.vocab_size))\n",
        "        f.write(struct.pack(\"I\", 2))  # bits\n",
        "        f.write(struct.pack(\"I\", 4))  # group_size\n",
        "        \n",
        "        # Quantize weights\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"weight\" in name and param.dim() >= 2:\n",
        "                # Quantize to ternary\n",
        "                weight = param.data.cpu().float()\n",
        "                scale = weight.abs().max(dim=-1, keepdim=True)[0]\n",
        "                normalized = weight / (scale + 1e-8)\n",
        "                \n",
        "                quantized = torch.zeros_like(normalized, dtype=torch.int8)\n",
        "                quantized[normalized > 0.5] = 1\n",
        "                quantized[normalized < -0.5] = -1\n",
        "                \n",
        "                # Pack and write\n",
        "                flat = quantized.flatten().numpy()\n",
        "                packed_len = (len(flat) + 3) // 4\n",
        "                packed = np.zeros(packed_len, dtype=np.uint8)\n",
        "                \n",
        "                for i in range(len(flat)):\n",
        "                    val = flat[i]\n",
        "                    encoded = 1 if val == 0 else (0 if val == -1 else 2)\n",
        "                    packed[i // 4] |= (encoded << ((i % 4) * 2))\n",
        "                \n",
        "                # Write tensor\n",
        "                name_bytes = name.encode()\n",
        "                f.write(struct.pack(\"I\", len(name_bytes)))\n",
        "                f.write(name_bytes)\n",
        "                f.write(struct.pack(\"I\", len(param.shape)))\n",
        "                for dim in param.shape:\n",
        "                    f.write(struct.pack(\"I\", dim))\n",
        "                f.write(packed.tobytes())\n",
        "                f.write(scale.squeeze(-1).numpy().tobytes())\n",
        "    \n",
        "    print(f\"Quantized model saved to {output_path}\")\n",
        "\n",
        "# Quantize\n",
        "OUTPUT_BITNET = \"model.tmac2.bin\"\n",
        "quantize_to_bitnet(merged_model, OUTPUT_BITNET)\n",
        "\n",
        "import os\n",
        "original_size = sum(p.numel() * 2 for p in merged_model.parameters())  # FP16\n",
        "quantized_size = os.path.getsize(OUTPUT_BITNET)\n",
        "print(f\"\\nOriginal size (FP16): {original_size / 1e6:.1f} MB\")\n",
        "print(f\"Quantized size: {quantized_size / 1e6:.1f} MB\")\n",
        "print(f\"Compression: {original_size / quantized_size:.1f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_header"
      },
      "source": [
        "## 8. Download Your Model\n",
        "\n",
        "Download the quantized model to deploy on your edge device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the quantized model\n",
        "files.download(OUTPUT_BITNET)\n",
        "print(f\"Downloaded {OUTPUT_BITNET}\")\n",
        "print(f\"\\nNext steps:\")\n",
        "print(f\"  1. Copy to your edge device (Raspberry Pi, Jetson, etc.)\")\n",
        "print(f\"  2. Run: edgellm serve --model {OUTPUT_BITNET} --port 8080\")\n",
        "print(f\"  3. Test: curl localhost:8080/v1/chat/completions ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Deploy to Edge Device:**\n",
        "   ```bash\n",
        "   # On your Raspberry Pi / edge device\n",
        "   edgellm serve --model model.tmac2.bin --port 8080\n",
        "   ```\n",
        "\n",
        "2. **Test the API:**\n",
        "   ```bash\n",
        "   curl localhost:8080/v1/chat/completions \\\n",
        "       -H \"Content-Type: application/json\" \\\n",
        "       -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Turn on the lights\"}]}'\n",
        "   ```\n",
        "\n",
        "3. **Benchmark:**\n",
        "   ```bash\n",
        "   edgellm benchmark --model model.tmac2.bin\n",
        "   ```\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [EdgeLLM GitHub](https://github.com/yourusername/edgellm)\n",
        "- [Documentation](https://github.com/yourusername/edgellm/docs)\n",
        "- [Hardware Guide](https://github.com/yourusername/edgellm/docs/hardware-guide.md)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
