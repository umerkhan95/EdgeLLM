{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EdgeLLM Flash Attention Benchmark\n",
    "\n",
    "This notebook benchmarks our Flash Attention implementation on Tesla T4 GPU.\n",
    "\n",
    "**Target:** Demonstrate 2-3x speedup over naive attention to help hit 630+ tok/s\n",
    "\n",
    "**Tests:**\n",
    "1. Correctness validation vs naive attention\n",
    "2. Forward pass performance at various sequence lengths\n",
    "3. Decode performance with KV cache (inference critical path)\n",
    "4. Memory efficiency comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,compute_cap,memory.total', \n",
    "                         '--format=csv,noheader'], capture_output=True, text=True)\n",
    "gpu_info = result.stdout.strip()\n",
    "print(f\"GPU: {gpu_info}\")\n",
    "\n",
    "# Parse compute capability\n",
    "parts = gpu_info.split(', ')\n",
    "GPU_NAME = parts[0] if len(parts) > 0 else 'Unknown'\n",
    "COMPUTE_CAP = parts[1] if len(parts) > 1 else '0.0'\n",
    "print(f\"Compute Capability: {COMPUTE_CAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!rm -rf ollama-api-gateway\n",
    "!git clone --depth 1 https://github.com/umerkhan95/ollama-api-gateway.git\n",
    "%cd ollama-api-gateway/mojo-gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Flash Attention for T4\n",
    "%cd src/kernels/cuda\n",
    "!make clean\n",
    "!make t4 flash\n",
    "print(\"\\nBuild complete!\")\n",
    "!ls -la ../../../lib/*.so 2>/dev/null || echo \"Checking for libraries...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and run Flash Attention tests\n",
    "!make test-flash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom_fa_bench.cu\n",
    "/**\n",
    " * Custom Flash Attention Benchmark for SmolLM-135M\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <chrono>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"flash_attention.h\"\n",
    "\n",
    "#define WARMUP 10\n",
    "#define RUNS 100\n",
    "\n",
    "void fill_random(float* data, int size) {\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\n=== SmolLM-135M Flash Attention Benchmark ===\");\n",
    "    printf(\"\\n\\nConfiguration:\\n\");\n",
    "    printf(\"  Model: SmolLM-135M\\n\");\n",
    "    printf(\"  Heads: 9\\n\");\n",
    "    printf(\"  Head dim: 64\\n\");\n",
    "    printf(\"  Hidden: 576\\n\\n\");\n",
    "    \n",
    "    // SmolLM-135M config\n",
    "    int batch = 1;\n",
    "    int num_heads = 9;\n",
    "    int head_dim = 64;\n",
    "    int batch_heads = batch * num_heads;\n",
    "    int num_layers = 9;\n",
    "    \n",
    "    // Initialize\n",
    "    flash_attention_init(batch, num_heads, 2048, head_dim);\n",
    "    flash_attention_init_kv_cache(batch, num_heads, 2048, head_dim);\n",
    "    \n",
    "    int single_size = batch_heads * head_dim;\n",
    "    float* Q = (float*)malloc(single_size * sizeof(float));\n",
    "    float* K = (float*)malloc(single_size * sizeof(float));\n",
    "    float* V = (float*)malloc(single_size * sizeof(float));\n",
    "    float* O = (float*)malloc(single_size * sizeof(float));\n",
    "    \n",
    "    srand(42);\n",
    "    fill_random(Q, single_size);\n",
    "    fill_random(K, single_size);\n",
    "    fill_random(V, single_size);\n",
    "    \n",
    "    // Fill cache to 256 tokens\n",
    "    for (int pos = 0; pos < 256; pos++) {\n",
    "        fill_random(K, single_size);\n",
    "        fill_random(V, single_size);\n",
    "        flash_attention_update_kv_cache(K, V, batch_heads, pos, 1, head_dim);\n",
    "    }\n",
    "    \n",
    "    printf(\"Decode benchmark (cache_len=256):\\n\");\n",
    "    printf(\"----------------------------------\\n\");\n",
    "    \n",
    "    // Warmup\n",
    "    for (int i = 0; i < WARMUP; i++) {\n",
    "        flash_attention_decode(Q, K, V, O, batch_heads, 255, head_dim);\n",
    "    }\n",
    "    \n",
    "    // Benchmark single attention layer\n",
    "    auto start = std::chrono::high_resolution_clock::now();\n",
    "    for (int i = 0; i < RUNS; i++) {\n",
    "        flash_attention_decode(Q, K, V, O, batch_heads, 255, head_dim);\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    double total_ms = std::chrono::duration<double, std::milli>(end - start).count();\n",
    "    double per_layer_ms = total_ms / RUNS;\n",
    "    double per_token_ms = per_layer_ms * num_layers;  // 9 layers\n",
    "    double tokens_per_sec = 1000.0 / per_token_ms;\n",
    "    \n",
    "    printf(\"  Per-layer attention: %.4f ms\\n\", per_layer_ms);\n",
    "    printf(\"  Per-token (9 layers): %.4f ms\\n\", per_token_ms);\n",
    "    printf(\"\\n\");\n",
    "    printf(\"Attention-only throughput: %.1f tok/s\\n\", tokens_per_sec);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // Estimate total inference throughput\n",
    "    // Attention is typically 30-40% of total inference time\n",
    "    double attention_fraction = 0.35;\n",
    "    double estimated_total_ms = per_token_ms / attention_fraction;\n",
    "    double estimated_throughput = 1000.0 / estimated_total_ms;\n",
    "    \n",
    "    printf(\"Estimated full inference throughput:\\n\");\n",
    "    printf(\"  (assuming attention = 35%% of compute)\\n\");\n",
    "    printf(\"  Per-token total: %.3f ms\\n\", estimated_total_ms);\n",
    "    printf(\"  Throughput: %.1f tok/s\\n\", estimated_throughput);\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    printf(\"Targets:\\n\");\n",
    "    printf(\"  Current target: 630 tok/s\\n\");\n",
    "    printf(\"  Ollama baseline: 423 tok/s\\n\");\n",
    "    printf(\"\\n\");\n",
    "    \n",
    "    // JSON output\n",
    "    printf(\"JSON Results:\\n\");\n",
    "    printf(\"{\\n\");\n",
    "    printf(\"  \\\"per_layer_attention_ms\\\": %.4f,\\n\", per_layer_ms);\n",
    "    printf(\"  \\\"per_token_attention_ms\\\": %.4f,\\n\", per_token_ms);\n",
    "    printf(\"  \\\"attention_throughput\\\": %.1f,\\n\", tokens_per_sec);\n",
    "    printf(\"  \\\"estimated_total_throughput\\\": %.1f,\\n\", estimated_throughput);\n",
    "    printf(\"  \\\"target_throughput\\\": 630,\\n\");\n",
    "    printf(\"  \\\"ollama_baseline\\\": 423\\n\");\n",
    "    printf(\"}\\n\");\n",
    "    \n",
    "    free(Q);\n",
    "    free(K);\n",
    "    free(V);\n",
    "    free(O);\n",
    "    flash_attention_cleanup();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and run custom benchmark\n",
    "!nvcc -O3 -gencode arch=compute_75,code=sm_75 \\\n",
    "    -o custom_fa_bench custom_fa_bench.cu flash_attention.o -lcudart\n",
    "!./custom_fa_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare: Flash Attention vs Naive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile compare_attention.cu\n",
    "/**\n",
    " * Compare Flash Attention vs Naive Attention\n",
    " */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <chrono>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"flash_attention.h\"\n",
    "\n",
    "// Naive attention kernel (for comparison)\n",
    "__global__ void naive_attention_kernel(\n",
    "    const float* Q, const float* K, const float* V, float* O,\n",
    "    int seq_len, int head_dim, float scale\n",
    ") {\n",
    "    int row = blockIdx.x;\n",
    "    int tid = threadIdx.x;\n",
    "    \n",
    "    if (row >= seq_len) return;\n",
    "    \n",
    "    extern __shared__ float smem[];\n",
    "    float* scores = smem;  // [seq_len]\n",
    "    \n",
    "    // Compute Q[row] @ K^T\n",
    "    for (int j = tid; j < seq_len; j += blockDim.x) {\n",
    "        float dot = 0.0f;\n",
    "        for (int d = 0; d < head_dim; d++) {\n",
    "            dot += Q[row * head_dim + d] * K[j * head_dim + d];\n",
    "        }\n",
    "        scores[j] = dot * scale;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find max (reduction)\n",
    "    float max_val = -1e9f;\n",
    "    for (int j = tid; j < seq_len; j += blockDim.x) {\n",
    "        max_val = fmaxf(max_val, scores[j]);\n",
    "    }\n",
    "    __shared__ float smax;\n",
    "    if (tid == 0) smax = -1e9f;\n",
    "    __syncthreads();\n",
    "    atomicMax((int*)&smax, __float_as_int(max_val));\n",
    "    __syncthreads();\n",
    "    max_val = smax;\n",
    "    \n",
    "    // Softmax\n",
    "    float sum = 0.0f;\n",
    "    for (int j = tid; j < seq_len; j += blockDim.x) {\n",
    "        scores[j] = expf(scores[j] - max_val);\n",
    "        sum += scores[j];\n",
    "    }\n",
    "    __shared__ float ssum;\n",
    "    if (tid == 0) ssum = 0.0f;\n",
    "    __syncthreads();\n",
    "    atomicAdd(&ssum, sum);\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int j = tid; j < seq_len; j += blockDim.x) {\n",
    "        scores[j] /= ssum;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // Output = scores @ V\n",
    "    for (int d = tid; d < head_dim; d += blockDim.x) {\n",
    "        float val = 0.0f;\n",
    "        for (int j = 0; j < seq_len; j++) {\n",
    "            val += scores[j] * V[j * head_dim + d];\n",
    "        }\n",
    "        O[row * head_dim + d] = val;\n",
    "    }\n",
    "}\n",
    "\n",
    "void fill_random(float* data, int size) {\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        data[i] = ((float)rand() / RAND_MAX - 0.5f) * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"\\n=== Flash Attention vs Naive Attention ===\");\n",
    "    printf(\"\\n\\n\");\n",
    "    \n",
    "    int batch_heads = 9;\n",
    "    int head_dim = 64;\n",
    "    int seq_lengths[] = {64, 128, 256, 512};\n",
    "    int num_tests = 4;\n",
    "    \n",
    "    printf(\"%-10s | %-12s | %-12s | %-10s\\n\",\n",
    "           \"Seq Len\", \"Naive (ms)\", \"Flash (ms)\", \"Speedup\");\n",
    "    printf(\"-----------|--------------|--------------|------------\\n\");\n",
    "    \n",
    "    for (int t = 0; t < num_tests; t++) {\n",
    "        int seq_len = seq_lengths[t];\n",
    "        int size = batch_heads * seq_len * head_dim;\n",
    "        \n",
    "        float* h_Q = (float*)malloc(size * sizeof(float));\n",
    "        float* h_K = (float*)malloc(size * sizeof(float));\n",
    "        float* h_V = (float*)malloc(size * sizeof(float));\n",
    "        float* h_O = (float*)malloc(size * sizeof(float));\n",
    "        \n",
    "        fill_random(h_Q, size);\n",
    "        fill_random(h_K, size);\n",
    "        fill_random(h_V, size);\n",
    "        \n",
    "        float *d_Q, *d_K, *d_V, *d_O;\n",
    "        cudaMalloc(&d_Q, size * sizeof(float));\n",
    "        cudaMalloc(&d_K, size * sizeof(float));\n",
    "        cudaMalloc(&d_V, size * sizeof(float));\n",
    "        cudaMalloc(&d_O, size * sizeof(float));\n",
    "        \n",
    "        cudaMemcpy(d_Q, h_Q, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(d_K, h_K, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(d_V, h_V, size * sizeof(float), cudaMemcpyHostToDevice);\n",
    "        \n",
    "        float scale = 1.0f / sqrtf(head_dim);\n",
    "        \n",
    "        // Benchmark naive\n",
    "        for (int i = 0; i < 5; i++) {\n",
    "            for (int bh = 0; bh < batch_heads; bh++) {\n",
    "                naive_attention_kernel<<<seq_len, 256, seq_len * sizeof(float)>>>(\n",
    "                    d_Q + bh * seq_len * head_dim,\n",
    "                    d_K + bh * seq_len * head_dim,\n",
    "                    d_V + bh * seq_len * head_dim,\n",
    "                    d_O + bh * seq_len * head_dim,\n",
    "                    seq_len, head_dim, scale);\n",
    "            }\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        \n",
    "        auto start = std::chrono::high_resolution_clock::now();\n",
    "        for (int i = 0; i < 50; i++) {\n",
    "            for (int bh = 0; bh < batch_heads; bh++) {\n",
    "                naive_attention_kernel<<<seq_len, 256, seq_len * sizeof(float)>>>(\n",
    "                    d_Q + bh * seq_len * head_dim,\n",
    "                    d_K + bh * seq_len * head_dim,\n",
    "                    d_V + bh * seq_len * head_dim,\n",
    "                    d_O + bh * seq_len * head_dim,\n",
    "                    seq_len, head_dim, scale);\n",
    "            }\n",
    "        }\n",
    "        cudaDeviceSynchronize();\n",
    "        auto end = std::chrono::high_resolution_clock::now();\n",
    "        double naive_ms = std::chrono::duration<double, std::milli>(end - start).count() / 50;\n",
    "        \n",
    "        // Benchmark Flash Attention\n",
    "        flash_attention_init(1, batch_heads, seq_len, head_dim);\n",
    "        \n",
    "        for (int i = 0; i < 5; i++) {\n",
    "            flash_attention_forward(h_Q, h_K, h_V, h_O, batch_heads, seq_len, head_dim, 1);\n",
    "        }\n",
    "        \n",
    "        start = std::chrono::high_resolution_clock::now();\n",
    "        for (int i = 0; i < 50; i++) {\n",
    "            flash_attention_forward(h_Q, h_K, h_V, h_O, batch_heads, seq_len, head_dim, 1);\n",
    "        }\n",
    "        end = std::chrono::high_resolution_clock::now();\n",
    "        double flash_ms = std::chrono::duration<double, std::milli>(end - start).count() / 50;\n",
    "        \n",
    "        double speedup = naive_ms / flash_ms;\n",
    "        \n",
    "        printf(\"%-10d | %-12.3f | %-12.3f | %-10.2fx\\n\",\n",
    "               seq_len, naive_ms, flash_ms, speedup);\n",
    "        \n",
    "        cudaFree(d_Q);\n",
    "        cudaFree(d_K);\n",
    "        cudaFree(d_V);\n",
    "        cudaFree(d_O);\n",
    "        free(h_Q);\n",
    "        free(h_K);\n",
    "        free(h_V);\n",
    "        free(h_O);\n",
    "        flash_attention_cleanup();\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -O3 -gencode arch=compute_75,code=sm_75 \\\n",
    "    -o compare_attention compare_attention.cu flash_attention.o -lcudart\n",
    "!./compare_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"gpu\": GPU_NAME,\n",
    "    \"compute_capability\": COMPUTE_CAP,\n",
    "    \"benchmark\": \"Flash Attention\",\n",
    "    \"implementation\": \"EdgeLLM custom CUDA\",\n",
    "    \"features\": [\n",
    "        \"Tiled computation\",\n",
    "        \"Online softmax (O(N) memory)\",\n",
    "        \"KV cache support\",\n",
    "        \"Causal masking\",\n",
    "        \"Optimized for SmolLM-135M (head_dim=64)\"\n",
    "    ],\n",
    "    \"target_throughput\": 630,\n",
    "    \"ollama_baseline\": 423\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "with open('flash_attention_benchmark_report.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"\\nReport saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f custom_fa_bench compare_attention custom_fa_bench.cu compare_attention.cu\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
