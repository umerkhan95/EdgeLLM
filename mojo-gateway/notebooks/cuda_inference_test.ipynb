{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EdgeLLM CUDA Inference Test\n",
        "\n",
        "This notebook tests the CUDA T-MAC kernels for GPU-accelerated BitNet inference.\n",
        "\n",
        "**Requirements:**\n",
        "- NVIDIA GPU (Jetson, RTX, etc.)\n",
        "- CUDA Toolkit 11.0+\n",
        "- nvcc compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check NVIDIA GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check CUDA version\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get GPU details\n",
        "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository and Build CUDA Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (if not already present)\n",
        "import os\n",
        "if not os.path.exists('ollama-api-gateway'):\n",
        "    !git clone https://github.com/umerkhan95/ollama-api-gateway.git\n",
        "else:\n",
        "    print('Repository already exists, pulling latest changes...')\n",
        "    !cd ollama-api-gateway && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Navigate to kernels directory\n",
        "%cd ollama-api-gateway/mojo-gateway/src/kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CUDA kernels\n",
        "!make cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify build output\n",
        "!ls -la ../../lib/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run CUDA Kernel Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run CUDA unit tests\n",
        "!make cuda-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Python CUDA Kernel Test\n",
        "\n",
        "Test the CUDA kernels directly from Python using ctypes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ctypes\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Find the CUDA library\n",
        "lib_path = '../../lib/libtmac_kernel_cuda.so'\n",
        "if not os.path.exists(lib_path):\n",
        "    raise FileNotFoundError(f'CUDA library not found at {lib_path}. Run make cuda first.')\n",
        "\n",
        "# Load the library\n",
        "cuda_lib = ctypes.CDLL(lib_path)\n",
        "print(f'Loaded CUDA library: {lib_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define function signatures\n",
        "cuda_lib.cuda_available.restype = ctypes.c_int\n",
        "cuda_lib.cuda_device_name.restype = ctypes.c_char_p\n",
        "cuda_lib.cuda_init.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int]\n",
        "cuda_lib.cuda_init.restype = ctypes.c_int\n",
        "cuda_lib.cuda_cleanup.restype = None\n",
        "\n",
        "# Check CUDA availability\n",
        "if cuda_lib.cuda_available():\n",
        "    device_name = cuda_lib.cuda_device_name().decode('utf-8')\n",
        "    print(f'CUDA Available: Yes')\n",
        "    print(f'Device: {device_name}')\n",
        "else:\n",
        "    print('CUDA Not Available')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CUDA\n",
        "max_weights = 10_000_000  # 10MB\n",
        "max_activations = 1_000_000\n",
        "max_output = 1_000_000\n",
        "\n",
        "ret = cuda_lib.cuda_init(max_weights, max_activations, max_output)\n",
        "if ret == 0:\n",
        "    print('CUDA initialized successfully')\n",
        "else:\n",
        "    print('CUDA initialization failed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test RMSNorm kernel\n",
        "cuda_lib.rmsnorm_cuda.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),  # output\n",
        "    ctypes.POINTER(ctypes.c_float),  # input\n",
        "    ctypes.POINTER(ctypes.c_float),  # weight\n",
        "    ctypes.c_int,                     # batch_size\n",
        "    ctypes.c_int,                     # size\n",
        "    ctypes.c_float                    # eps\n",
        "]\n",
        "cuda_lib.rmsnorm_cuda.restype = ctypes.c_int\n",
        "\n",
        "# Create test data\n",
        "batch_size = 4\n",
        "size = 256\n",
        "\n",
        "input_data = np.random.randn(batch_size, size).astype(np.float32)\n",
        "weight_data = np.ones(size, dtype=np.float32)\n",
        "output_data = np.zeros((batch_size, size), dtype=np.float32)\n",
        "\n",
        "# Run RMSNorm on GPU\n",
        "ret = cuda_lib.rmsnorm_cuda(\n",
        "    output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "    input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "    weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "    batch_size,\n",
        "    size,\n",
        "    ctypes.c_float(1e-6)\n",
        ")\n",
        "\n",
        "if ret == 0:\n",
        "    print('RMSNorm CUDA: SUCCESS')\n",
        "    print(f'Input mean: {input_data.mean():.4f}')\n",
        "    print(f'Output mean: {output_data.mean():.4f}')\n",
        "    print(f'Output std: {output_data.std():.4f}')\n",
        "else:\n",
        "    print('RMSNorm CUDA: FAILED')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Softmax kernel\n",
        "cuda_lib.softmax_cuda.argtypes = [\n",
        "    ctypes.POINTER(ctypes.c_float),  # output\n",
        "    ctypes.POINTER(ctypes.c_float),  # input\n",
        "    ctypes.c_int,                     # batch_size\n",
        "    ctypes.c_int                      # size\n",
        "]\n",
        "cuda_lib.softmax_cuda.restype = ctypes.c_int\n",
        "\n",
        "# Create test data\n",
        "logits = np.random.randn(batch_size, size).astype(np.float32) * 2\n",
        "probs = np.zeros((batch_size, size), dtype=np.float32)\n",
        "\n",
        "# Run Softmax on GPU\n",
        "ret = cuda_lib.softmax_cuda(\n",
        "    probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "    logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "    batch_size,\n",
        "    size\n",
        ")\n",
        "\n",
        "if ret == 0:\n",
        "    print('Softmax CUDA: SUCCESS')\n",
        "    # Verify softmax sums to 1\n",
        "    for b in range(batch_size):\n",
        "        row_sum = probs[b].sum()\n",
        "        print(f'  Batch {b} sum: {row_sum:.6f} (should be ~1.0)')\n",
        "else:\n",
        "    print('Softmax CUDA: FAILED')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Benchmark RMSNorm\n",
        "batch_size = 32\n",
        "size = 4096  # Typical hidden size\n",
        "iterations = 1000\n",
        "\n",
        "input_data = np.random.randn(batch_size, size).astype(np.float32)\n",
        "weight_data = np.ones(size, dtype=np.float32)\n",
        "output_data = np.zeros((batch_size, size), dtype=np.float32)\n",
        "\n",
        "# Warmup\n",
        "for _ in range(10):\n",
        "    cuda_lib.rmsnorm_cuda(\n",
        "        output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        batch_size, size, ctypes.c_float(1e-6)\n",
        "    )\n",
        "\n",
        "# Benchmark\n",
        "cuda_lib.cuda_sync()  # Ensure warmup is done\n",
        "start = time.perf_counter()\n",
        "for _ in range(iterations):\n",
        "    cuda_lib.rmsnorm_cuda(\n",
        "        output_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        input_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        weight_data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        batch_size, size, ctypes.c_float(1e-6)\n",
        "    )\n",
        "cuda_lib.cuda_sync()\n",
        "end = time.perf_counter()\n",
        "\n",
        "total_time = end - start\n",
        "per_call = total_time / iterations * 1000  # ms\n",
        "throughput = iterations / total_time\n",
        "\n",
        "print(f'RMSNorm Benchmark ({batch_size}x{size}):')\n",
        "print(f'  Total time: {total_time:.3f}s for {iterations} iterations')\n",
        "print(f'  Per call: {per_call:.3f}ms')\n",
        "print(f'  Throughput: {throughput:.1f} calls/sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark Softmax\n",
        "logits = np.random.randn(batch_size, size).astype(np.float32)\n",
        "probs = np.zeros((batch_size, size), dtype=np.float32)\n",
        "\n",
        "# Warmup\n",
        "for _ in range(10):\n",
        "    cuda_lib.softmax_cuda(\n",
        "        probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        batch_size, size\n",
        "    )\n",
        "\n",
        "# Benchmark\n",
        "cuda_lib.cuda_sync()\n",
        "start = time.perf_counter()\n",
        "for _ in range(iterations):\n",
        "    cuda_lib.softmax_cuda(\n",
        "        probs.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        logits.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n",
        "        batch_size, size\n",
        "    )\n",
        "cuda_lib.cuda_sync()\n",
        "end = time.perf_counter()\n",
        "\n",
        "total_time = end - start\n",
        "per_call = total_time / iterations * 1000\n",
        "throughput = iterations / total_time\n",
        "\n",
        "print(f'Softmax Benchmark ({batch_size}x{size}):')\n",
        "print(f'  Total time: {total_time:.3f}s for {iterations} iterations')\n",
        "print(f'  Per call: {per_call:.3f}ms')\n",
        "print(f'  Throughput: {throughput:.1f} calls/sec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "cuda_lib.cuda_cleanup()\n",
        "print('CUDA resources cleaned up')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "This notebook tested:\n",
        "1. CUDA environment detection\n",
        "2. Building CUDA kernels\n",
        "3. RMSNorm kernel functionality\n",
        "4. Softmax kernel functionality\n",
        "5. Performance benchmarks\n",
        "\n",
        "**Next Steps:**\n",
        "- Test T-MAC matmul kernel with real model weights\n",
        "- Compare performance vs CPU (AVX2/NEON)\n",
        "- Run full inference pipeline"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
